{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR_Opportunity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GmKJ_8MezAE"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooQIl2_We4DH"
      },
      "source": [
        "This notebook presents the several machine learning models using CNN and LSTM for HAR. To obtain a detailed description of the architecture, please refer to the dissertation, **\"RECOGNISING HUMAN ACTIVITIES AUTONOMOUSLY THROUGH FUSION OF SENSOR DATA\"**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aylTQ4axf1Tj"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faDVCZUmf5UG"
      },
      "source": [
        "As a dataset, the [OPPORTUNITY Activity Recognition Data Set](http://archive.ics.uci.edu/ml/datasets/OPPORTUNITY+Activity+Recognition) is used. To prepare this dataset for the program, the following code is uncommented and executed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgr2ZLV5MJF8"
      },
      "source": [
        "# Download dataset zip file and place data files in training and test set directries\n",
        "\n",
        "zipfile_dataset_opportunity = \"OpportunityUCIDataset.zip\"\n",
        "url_dataset_opportunity = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip\"\n",
        "\n",
        "#!wget $url_dataset_opportunity\n",
        "#!unzip $zipfile_dataset_opportunity\n",
        "#!ls OpportunityUCIDataset/dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PdJC75UMzqQ"
      },
      "source": [
        "# Deploy dataset files into training and test directories\n",
        "\n",
        "#!mkdir -p ../data/test\n",
        "#!mkdir ../data/train\n",
        "\n",
        "#%cd OpportunityUCIDataset/dataset/\n",
        "#!cp S[1-3]-Drill.dat S1-ADL[1-5].dat S2-ADL[1-3].dat S3-ADL[1-3].dat ../../../data/train/\n",
        "#!cp S[23]-ADL[23].dat ../../../data/test/\n",
        "#%cd ../../\n",
        "#!ls ../data/train/\n",
        "#!ls ../data/test/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmCYvllE_gnC"
      },
      "source": [
        "# 1.Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiM8QQLChLmL"
      },
      "source": [
        "Adjustable flags and parameters are listed. Hyperparameters for each ML model are in \"[F] ML models\" section.\n",
        "\n",
        "|Name|Type|Explanation|\n",
        "|-|-|-|\n",
        "|flag_delete_null|Flag|Whether delete the Null class or not|\n",
        "|flag_label|Flag|Activity type (gesture or locomotion)|\n",
        "|flag_(ML model name)|Flag|Whether execute the model or not|\n",
        "|flag_experiment|Flag|Whether run repeated evaluation for summary statistics or not|\n",
        "|flag_model_load|Flag|Whether load the model from the file or not|\n",
        "|flag_model_save|Flag|Whether save the model to the file after training or not|\n",
        "|flag_EarlyStopping|Flag|Enable Early stopping|\n",
        "|flag_es_monitor|Flag|Monitor type for Early stopping|\n",
        "|ratio_train|Parameter|The ratio between training and validation sets|\n",
        "|seed|Parameter|Fix the seed for reproducibility|\n",
        "|flag_scale|Flag|Scaling technique|\n",
        "|window_size|Parameter|The length of the sliding window|\n",
        "|window_step|Parameter|The step of the sliding window|\n",
        "|flag_sw_label|Flag|Class label of the sliding window|\n",
        "|flag_balance_*|Flag|Enable data balancing|\n",
        "|flag_data_dist|Flag|Display dataset distribution|\n",
        "|flag_interpolate|Flag|Enable interpolation|\n",
        "|flag_plot_model|Flag|Whether plot model graphs or not|\n",
        "|flag_save_fig|Flag|Whether save graphs or not|\n",
        "|flag_summary|Flag|Show summary of the dataset|\n",
        "|flag_cm_norm|Flag|Whether normalise confusion matrix or not|\n",
        "|flag_TensorBoard|Flag|Whether save the Tensorboard data or not|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGmRyjGiCqFE"
      },
      "source": [
        "### [Note]\n",
        "#\n",
        "# Hyperparameters for each ML model are in \"[F] ML models\" section\n",
        "#\n",
        "### ---------- ---------- ---------- ---------- ----------\n",
        "### Flags\n",
        "flag_delete_null    = False\n",
        "\n",
        "# Label\n",
        "flag_label          = \"ML_Both_Arms\"\n",
        "#flag_label          = \"Locomotion\"\n",
        "\n",
        "# ML\n",
        "flag_CNN_1d         = True\n",
        "flag_LSTM_Mto1      = True\n",
        "flag_CNN1D_LSTM     = True\n",
        "flag_ConvLSTM       = True\n",
        "flag_Ensemble       = True\n",
        "\n",
        "flag_experiment     = True\n",
        "\n",
        "flag_model_load     = False\n",
        "flag_model_save     = True\n",
        "\n",
        "flag_EarlyStopping  = True\n",
        "flag_es_monitor     = \"val_loss\"\n",
        "#flag_es_monitor     = \"val_accuracy\"\n",
        "### ---------- ---------- ---------- ---------- ----------\n",
        "### Pre-processing\n",
        "# Ratio of training dataset to be split\n",
        "ratio_train = 0.85\n",
        "\n",
        "# Randam seed for reproducibility\n",
        "seed = 7\n",
        "\n",
        "# scaling\n",
        "flag_scaling        = \"Std\" # for Gaussian\n",
        "#flag_scaling        = \"Norm\" # (0 - 1)\n",
        "\n",
        "# Sliding window\n",
        "window_size = 15\n",
        "window_step = 8\n",
        "\n",
        "flag_sw_label       = \"last\"\n",
        "#flag_sw_label       = \"mode\"\n",
        "\n",
        "# Data balancing\n",
        "flag_balance_under1 = False\n",
        "flag_balance_under2 = False\n",
        "flag_balance_under3 = False\n",
        "\n",
        "flag_balance_over1  = False\n",
        "flag_balance_over2  = False\n",
        "flag_balance_over3  = False\n",
        "### ---------- ---------- ---------- ---------- ----------\n",
        "### Evaluation\n",
        "flag_data_dist      = False\n",
        "flag_interpolate    = True\n",
        "flag_plot_model     = True\n",
        "flag_savefig        = True\n",
        "\n",
        "flag_summary        = True\n",
        "flag_cm_norm        = True\n",
        "\n",
        "flag_TensorBoard    = False\n",
        "### ---------- ---------- ---------- ---------- ----------\n",
        "### Directories\n",
        "dir_log = 'log'\n",
        "dir_model = 'model'\n",
        "### ---------- ---------- ---------- ---------- ----------\n",
        "### Names\n",
        "# models\n",
        "modelname_cnn_1d         = 'CNN_1D'\n",
        "modelname_lstm_Mto1      = 'LSTM_Mto1'\n",
        "modelname_cnn1d_lstm     = 'CNN1D_LSTM'\n",
        "modelname_convlstm       = 'ConvLSTM'\n",
        "modelname_ensemble       = 'Ensemble'\n",
        "modelname_lstm_Mto1_null = 'LSTM_Mto1_null'\n",
        "\n",
        "# Label list\n",
        "labels_Loco = ['(Null)',\n",
        "               'Stand',\n",
        "               'Walk',\n",
        "               'Sit',\n",
        "               'Lie']\n",
        "\n",
        "labels_ML = ['(Null)',\n",
        "             'Open Door 1',     'Open Door 2',\n",
        "             'Close Door 1',    'Close Door 2',\n",
        "             'Open Fridge',     'Close Fridge',\n",
        "             'Open Dishwasher', 'Close Dishwasher',\n",
        "             'Open Drawer 1',   'Close Drawer 1',\n",
        "             'Open Drawer 2',   'Close Drawer 2',\n",
        "             'Open Drawer 3',   'Close Drawer 3',\n",
        "             'Clean Table',     'Drink from Cup', 'Toggle Switch']\n",
        "### ---------- ---------- ---------- ---------- ----------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EysX8aadPofF"
      },
      "source": [
        "# 2.Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z2SyAlvgjaF"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axdkz_2aIA2t"
      },
      "source": [
        "# Pre-process\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import collections\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from datetime import datetime\n",
        "from numpy import mean, std\n",
        "from matplotlib import pyplot\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# NNs\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "\n",
        "# CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPool1D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.layers import Input, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# LSTM\n",
        "from tensorflow.keras.layers import LSTM, TimeDistributed\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# ConvLSTM\n",
        "from tensorflow.keras.layers import ConvLSTM2D\n",
        "\n",
        "# Ensemble\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdVQya14Q3t1"
      },
      "source": [
        "# Set random seed (for reproducibility)\n",
        "# Hash seed\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# Built-in random\n",
        "rn.seed(seed)\n",
        "\n",
        "# Numpy.random\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Tensorflow\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfwP11LziJOI"
      },
      "source": [
        "## [F] Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8LPit8mYM5F"
      },
      "source": [
        "def read_files(files):\n",
        "    for i, file in enumerate(files):\n",
        "        print(f'[{i+1}] Reading file: {file}')\n",
        "        d = pd.read_csv(file, header=None, sep=' ')\n",
        "\n",
        "        # Truncate last residual records for sliding window\n",
        "        mod = (len(d) - window_size) % window_step\n",
        "        d = d[:len(d)-mod]\n",
        "\n",
        "        # Count records with NaN\n",
        "        d_nan = d.isnull().sum()\n",
        "\n",
        "        # Convert NaN\n",
        "        # Linear interpolation\n",
        "        if flag_interpolate:\n",
        "            d.interpolate(inplace=True)\n",
        "\n",
        "        # Convert remaining NaNs into 0\n",
        "        if flag_interpolate:\n",
        "            d.replace(np.nan, 0, inplace=True)\n",
        "\n",
        "        if i == 0:\n",
        "            dataset = d\n",
        "            dataset_nan = d_nan\n",
        "        else:\n",
        "            dataset = pd.concat([dataset, d])\n",
        "            dataset_nan = dataset_nan + d_nan\n",
        "            \n",
        "    return dataset, dataset_nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx3NDhvNN8JU"
      },
      "source": [
        "# Adjust label values (0 to num_classes)\n",
        "def adjust_idx_labels(data_y):\n",
        "    if flag_label == 'Locomotion':\n",
        "        data_y[data_y == 4] = 3\n",
        "        data_y[data_y == 5] = 4\n",
        "    elif flag_label == 'ML_Both_Arms':\n",
        "        data_y[data_y == 406516] = 1\n",
        "        data_y[data_y == 406517] = 2\n",
        "        data_y[data_y == 404516] = 3\n",
        "        data_y[data_y == 404517] = 4\n",
        "        data_y[data_y == 406520] = 5\n",
        "        data_y[data_y == 404520] = 6\n",
        "        data_y[data_y == 406505] = 7\n",
        "        data_y[data_y == 404505] = 8\n",
        "        data_y[data_y == 406519] = 9\n",
        "        data_y[data_y == 404519] = 10\n",
        "        data_y[data_y == 406511] = 11\n",
        "        data_y[data_y == 404511] = 12\n",
        "        data_y[data_y == 406508] = 13\n",
        "        data_y[data_y == 404508] = 14\n",
        "        data_y[data_y == 408512] = 15\n",
        "        data_y[data_y == 407521] = 16\n",
        "        data_y[data_y == 405506] = 17\n",
        "\n",
        "    return data_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pbV9UNjwMVw"
      },
      "source": [
        "def sliding_window(data, w_size, w_step):\n",
        "    shape = np.array(data.shape)\n",
        "\n",
        "    # Compute new shape & strides based on window size & step\n",
        "    newshape = ((shape - w_size) // w_step) + 1\n",
        "    newshape = np.append(newshape, [w_size[0], data.shape[1]])\n",
        "\n",
        "    # Original strides * window step\n",
        "    newstrides = np.array(data.strides) * w_step\n",
        "\n",
        "    # For window size & features, set original strides\n",
        "    newstrides = np.append(newstrides, data.strides)\n",
        "\n",
        "    # Create a view for new shape & stride\n",
        "    data_strided = as_strided(data, shape=newshape, strides=newstrides)\n",
        "\n",
        "    # Flatten strided shape\n",
        "    newshape_flatten = [i for i in newshape if i != 1]\n",
        "\n",
        "    return data_strided.reshape(newshape_flatten)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ1WJKxNwaMh"
      },
      "source": [
        "def opp_sliding_window(X, Y):\n",
        "    X = sliding_window(X, (window_size, X.shape[1]), (window_step, 1))\n",
        "    Y = sliding_window(Y, (window_size, Y.shape[1]), (window_step, 1))\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG3wuJis34qR"
      },
      "source": [
        "## [F] ML models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoCODptk4qxw"
      },
      "source": [
        "# Train\n",
        "epochs = 100\n",
        "batch_size = 100\n",
        "repeats = 10\n",
        "\n",
        "# EarlyStopping\n",
        "es_patience = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOKqgOtTKMY4"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBwsKgbW6kxj"
      },
      "source": [
        "# Layer\n",
        "cnn_padding ='same'\n",
        "cnn_activation = 'relu'\n",
        "cnn_units = 128\n",
        "cnn_dropout = 0.5\n",
        "cnn_pool_size = 2\n",
        "\n",
        "## 1D Conv\n",
        "cnn_1d_filters = 64\n",
        "cnn_1d_kernel_size = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sM5-KLIugQ"
      },
      "source": [
        "def build_model_cnn_1d():\n",
        "    model = Sequential(name=modelname_cnn_1d)\n",
        "\n",
        "    # Conv layer 1\n",
        "    model.add(Conv1D(\n",
        "        input_shape = cnn_1d_input_shape,\n",
        "        filters     = cnn_1d_filters,\n",
        "        kernel_size = cnn_1d_kernel_size,\n",
        "        padding     = cnn_padding,\n",
        "        activation  = cnn_activation))\n",
        "\n",
        "    # Conv layer 2\n",
        "    model.add(Conv1D(\n",
        "        filters     = cnn_1d_filters,\n",
        "        kernel_size = cnn_1d_kernel_size,\n",
        "        padding     = cnn_padding,\n",
        "        activation  = cnn_activation))\n",
        "\n",
        "    # Conv layer 3\n",
        "    model.add(Conv1D(\n",
        "        filters     = cnn_1d_filters,\n",
        "        kernel_size = cnn_1d_kernel_size,\n",
        "        padding     = cnn_padding,\n",
        "        activation  = cnn_activation))\n",
        "\n",
        "    # Conv layer 4\n",
        "    model.add(Conv1D(\n",
        "        filters     = cnn_1d_filters,\n",
        "        kernel_size = cnn_1d_kernel_size,\n",
        "        padding     = cnn_padding,\n",
        "        activation  = cnn_activation))\n",
        "\n",
        "    # Maxpool layer\n",
        "    # model.add(MaxPool1D(\n",
        "    #     pool_size = cnn_pool_size))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense layer 1\n",
        "    model.add(Dense(\n",
        "        units      = cnn_units,\n",
        "        activation = 'relu'))\n",
        "\n",
        "    # Dropout\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    # Dense layer 2\n",
        "    model.add(Dense(\n",
        "        units      = cnn_units,\n",
        "        activation = 'relu'))\n",
        "\n",
        "    # Dropout\n",
        "    model.add(Dropout(cnn_dropout))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(\n",
        "        units      = num_classes,\n",
        "        activation = 'softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss       = 'categorical_crossentropy',\n",
        "        optimizer  = 'adam',\n",
        "        metrics    = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1xpyw17xvCP"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21V53fOV6oAe"
      },
      "source": [
        "# Layer\n",
        "lstm_units = 128\n",
        "lstm_dropout = 0.5\n",
        "lstm_weight_decay = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXNIOAt-IeaQ"
      },
      "source": [
        "# LSTM (Many-to-One, stateless)\n",
        "def build_model_lstm_Mto1():\n",
        "    model = Sequential(name=modelname_lstm_Mto1)\n",
        "\n",
        "    # LSTM layer\n",
        "    model.add(LSTM(\n",
        "        input_shape        = lstm_input_shape,\n",
        "        units              = lstm_units,\n",
        "        # kernel_regularizer = regularizers.l2(lstm_weight_decay),\n",
        "        return_sequences   = False)) # final layer of LSTM (only final output)\n",
        "\n",
        "    # Dropout\n",
        "    model.add(Dropout(lstm_dropout))\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(Dense(\n",
        "        units      = lstm_units,\n",
        "        activation = 'relu'))\n",
        "\n",
        "    # Dropout\n",
        "    model.add(Dropout(lstm_dropout))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(\n",
        "        units      = num_classes,\n",
        "        activation = 'softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss       = 'categorical_crossentropy',\n",
        "        optimizer  = 'adam',\n",
        "        metrics    = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zncSMHCRDJd"
      },
      "source": [
        "### CNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvEUT7LSE3SV"
      },
      "source": [
        "# Data\n",
        "cnn_lstm_steps = 3\n",
        "cnn_lstm_length = int(window_size / cnn_lstm_steps)\n",
        "\n",
        "# Layer\n",
        "cnn_lstm_padding = 'same'\n",
        "cnn_lstm_activation = 'relu'\n",
        "cnn_lstm_dropout = 0.5\n",
        "cnn_lstm_pool_size = 2\n",
        "\n",
        "## CNN\n",
        "cnn_lstm_filters = 64\n",
        "cnn1d_lstm_kernel_size = 3\n",
        "\n",
        "# LSTM\n",
        "cnn_lstm_units = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgdO9LzIRCg2"
      },
      "source": [
        "def build_model_cnn1d_lstm():\n",
        "    model = Sequential(name=modelname_cnn1d_lstm)\n",
        "\n",
        "    ## CNN (with TimeDistributed)\n",
        "    # Conv layer 1\n",
        "    model.add(TimeDistributed(Conv1D(\n",
        "        filters     = cnn_lstm_filters,\n",
        "        kernel_size = cnn1d_lstm_kernel_size,\n",
        "        padding     = cnn_lstm_padding,\n",
        "        activation  = cnn_lstm_activation),\n",
        "        input_shape = cnn1d_lstm_input_shape))\n",
        "\n",
        "    # Conv layer 2\n",
        "    model.add(TimeDistributed(Conv1D(\n",
        "        filters     = cnn_lstm_filters,\n",
        "        kernel_size = cnn1d_lstm_kernel_size,\n",
        "        padding     = cnn_lstm_padding,\n",
        "        activation  = cnn_lstm_activation)))\n",
        "\n",
        "    # Conv layer 3\n",
        "    model.add(TimeDistributed(Conv1D(\n",
        "        filters     = cnn_lstm_filters,\n",
        "        kernel_size = cnn1d_lstm_kernel_size,\n",
        "        padding     = cnn_lstm_padding,\n",
        "        activation  = cnn_lstm_activation)))\n",
        "\n",
        "    # Conv layer 4\n",
        "    model.add(TimeDistributed(Conv1D(\n",
        "        filters     = cnn_lstm_filters,\n",
        "        kernel_size = cnn1d_lstm_kernel_size,\n",
        "        padding     = cnn_lstm_padding,\n",
        "        activation  = cnn_lstm_activation)))\n",
        "\n",
        "    # Dropout\n",
        "    model.add(TimeDistributed(Dropout(cnn_lstm_dropout)))\n",
        "\n",
        "    # Maxpool layer\n",
        "    model.add(TimeDistributed(MaxPool1D(\n",
        "        pool_size = cnn_lstm_pool_size)))\n",
        "\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "    ## LSTM\n",
        "    # LSTM layer 1\n",
        "    model.add(LSTM(\n",
        "        units            = cnn_lstm_units,\n",
        "        return_sequences = True))\n",
        "\n",
        "    # Dropout\n",
        "    model.add(Dropout(cnn_lstm_dropout))\n",
        "\n",
        "    # LSTM layer 2\n",
        "    model.add(LSTM(\n",
        "        units            = cnn_lstm_units,\n",
        "        return_sequences = False))\n",
        "\n",
        "    # Dropout\n",
        "    model.add(Dropout(cnn_lstm_dropout))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(\n",
        "        units      = num_classes,\n",
        "        activation = 'softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss       = 'categorical_crossentropy',\n",
        "        optimizer  = 'adam',\n",
        "        metrics    = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLfIZcB0TVvV"
      },
      "source": [
        "### ConvLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DZU2MiDTYJn"
      },
      "source": [
        "# Data\n",
        "convlstm_steps = 3\n",
        "convlstm_length = int(window_size / convlstm_steps)\n",
        "\n",
        "# Layer\n",
        "convlstm_padding = 'same'\n",
        "convlstm_activation = 'relu'\n",
        "convlstm_dropout = 0.5\n",
        "convlstm_pool_size = 2\n",
        "\n",
        "## CNN\n",
        "convlstm_filters = 64\n",
        "convlstm_kernel_size = (1, 3)\n",
        "\n",
        "convlstm_units = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3v-Tb39UxOE"
      },
      "source": [
        "def build_model_convlstm():\n",
        "    model = Sequential(name=modelname_convlstm)\n",
        "\n",
        "    # Conv LSTM layer 1\n",
        "    model.add(ConvLSTM2D(\n",
        "        filters          = convlstm_filters,\n",
        "        kernel_size      = convlstm_kernel_size,\n",
        "        padding          = convlstm_padding,\n",
        "        activation       = convlstm_activation,\n",
        "        input_shape      = convlstm_input_shape,\n",
        "        return_sequences = True))\n",
        "        # return_sequences = False)) # final layer of LSTM (only final output)\n",
        "\n",
        "    # Conv LSTM layer 2\n",
        "    model.add(ConvLSTM2D(\n",
        "        filters          = convlstm_filters,\n",
        "        kernel_size      = convlstm_kernel_size,\n",
        "        padding          = convlstm_padding,\n",
        "        activation       = convlstm_activation,\n",
        "        return_sequences = True))\n",
        "\n",
        "    # Conv LSTM layer 3\n",
        "    model.add(ConvLSTM2D(\n",
        "        filters          = convlstm_filters,\n",
        "        kernel_size      = convlstm_kernel_size,\n",
        "        padding          = convlstm_padding,\n",
        "        activation       = convlstm_activation,\n",
        "        return_sequences = True))\n",
        "\n",
        "    # Conv LSTM layer 4\n",
        "    model.add(ConvLSTM2D(\n",
        "        filters          = convlstm_filters,\n",
        "        kernel_size      = convlstm_kernel_size,\n",
        "        padding          = convlstm_padding,\n",
        "        activation       = convlstm_activation,\n",
        "        return_sequences = False))\n",
        "\n",
        "    # Dropout\n",
        "    model.add(Dropout(convlstm_dropout))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(Dense(\n",
        "        units      = convlstm_units,\n",
        "        activation = convlstm_activation))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(\n",
        "        units      = num_classes,\n",
        "        activation = 'softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss       = 'categorical_crossentropy',\n",
        "        optimizer  = 'adam',\n",
        "        metrics    = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE3gj-xPDTC5"
      },
      "source": [
        "### Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXjgB_0wDf_x"
      },
      "source": [
        "# Layer\n",
        "ensemble_units = 10\n",
        "ensemble_activation = 'relu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfxC8c1RDRo6"
      },
      "source": [
        "def build_model_ensemble(inputs, outputs):\n",
        "    ensemble_merge = Concatenate(axis=1)(outputs)\n",
        "\n",
        "    # Dense layer\n",
        "    ensemble_hidden = Dense(\n",
        "        units      = ensemble_units,\n",
        "        activation = ensemble_activation)(ensemble_merge)\n",
        "\n",
        "    # Output layer\n",
        "    ensemble_output = Dense(\n",
        "        units      = num_classes,\n",
        "        activation = 'softmax')(ensemble_hidden)\n",
        "\n",
        "    model = Model(\n",
        "        inputs     = inputs,\n",
        "        outputs    = ensemble_output,\n",
        "        name       = modelname_ensemble)\n",
        "\n",
        "    model.compile(\n",
        "        loss       = 'categorical_crossentropy',\n",
        "        optimizer  = 'adam',\n",
        "        metrics    = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vRs98_f_qhj"
      },
      "source": [
        "## [F] Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjY63c0_hf5H"
      },
      "source": [
        "# Train and evaluate a model (once)\n",
        "def evaluate_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    # Build model\n",
        "    if model_name == modelname_cnn_1d:\n",
        "        model = build_model_cnn_1d()\n",
        "    elif model_name == modelname_lstm_Mto1:\n",
        "        model = build_model_lstm_Mto1()\n",
        "    elif model_name == modelname_cnn1d_lstm:\n",
        "        model = build_model_cnn1d_lstm()\n",
        "    elif model_name == modelname_convlstm:\n",
        "        model = build_model_convlstm()\n",
        "    else:\n",
        "        print(\"Error: specify correct model name\")\n",
        "        return -1\n",
        "    \n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        x               = X_train,\n",
        "        y               = y_train,\n",
        "        batch_size      = batch_size,\n",
        "        epochs          = epochs,\n",
        "        verbose         = 0,\n",
        "        callbacks       = [cb],\n",
        "        validation_data = (X_val, y_val)\n",
        "    )\n",
        "\n",
        "    num_epochs = len(history.history['loss'])\n",
        "\n",
        "    ## Evaluate\n",
        "    # Accuracy\n",
        "    _, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # F1\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test.argmax(axis=-1), y_pred.argmax(axis=-1), average='weighted')\n",
        "\n",
        "    return accuracy, f1, num_epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ0b9G-Mgbxi"
      },
      "source": [
        "# Repeat experiment\n",
        "def run_experiment(model_name, X_train, X_val, X_test, y_train, y_val, y_test, repeats=10):\n",
        "    print(f'Model: {model_name}')\n",
        "\n",
        "    scores_acc = []\n",
        "    scores_f1 = []\n",
        "    scores_epoch = []\n",
        "    for r in range(repeats):\n",
        "        acc, f1, epoch = evaluate_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "        print(f'[#{r+1:>2d}] Accuracy: {acc:.3f}, F1 score(weighted): {f1:.3f}, epoch: {epoch}')\n",
        "        scores_acc.append(acc)\n",
        "        scores_f1.append(f1)\n",
        "        scores_epoch.append(epoch)\n",
        "    \n",
        "    # Summarise mean and standard deviation\n",
        "    print(f'Accuracy: {mean(scores_acc):.3f} (+/- {std(scores_acc):.3f})')\n",
        "    print(f'F1 score(weighted): {mean(scores_f1):.3f} (+/- {std(scores_f1):.3f})')\n",
        "    print(f'epoch: {mean(scores_epoch):.1f} (+/- {std(scores_epoch):.3f})')\n",
        "\n",
        "\t# Boxplot of scores\n",
        "    metrics_list = ['Accuracy', 'F1 score']\n",
        "    all_scores = []\n",
        "    all_scores.append(scores_acc)\n",
        "    all_scores.append(scores_f1)\n",
        "    plt.boxplot(all_scores, labels=metrics_list)\n",
        "\n",
        "    if flag_savefig:\n",
        "        plt.savefig(\"boxplot_\" + model_name + \".png\")\n",
        "        \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vf8LExnjGUe"
      },
      "source": [
        "# Plot a histogram of each variable in the dataset\n",
        "def plot_variable_distributions(X, start=0, end=None, xlim=None):\n",
        "    if end is None:\n",
        "        end = X.shape[1]-1\n",
        "\n",
        "    print(X.shape)\n",
        "    num_features = end - start +1\n",
        "    print(f'# of plots: {num_features} ({start} - {end})')\n",
        "\n",
        "    plt.figure(figsize=(10, 2*num_features), tight_layout=True)\n",
        "    xaxis = None\n",
        "    for i, f in enumerate(range(start, end+1)):\n",
        "        print(i)\n",
        "        if xlim is None:\n",
        "            ax = plt.subplot(num_features, 1, i+1, title='Feature: ' + str(f))\n",
        "        else:\n",
        "            ax = plt.subplot(num_features, 1, i+1, sharex=xaxis, title='Feature: ' + str(f))\n",
        "            ax.set_xlim(xlim)\n",
        "        if i == 0:\n",
        "            xaxis = ax\n",
        "        plt.hist(X[:, f], bins=100)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NxZ4CUd_SLc"
      },
      "source": [
        "# Plot graphs for loss and accuracy\n",
        "def plot_acc_graph(history):\n",
        "\n",
        "    # Set figure size\n",
        "    fig = plt.figure(figsize=(15, 6))\n",
        "    plt.subplots_adjust(wspace=0.2)\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['loss'],\n",
        "            label='Train',\n",
        "            color='black')\n",
        "\n",
        "    plt.plot(history.history['val_loss'],\n",
        "            label='Val',\n",
        "            color='red')\n",
        "\n",
        "    #plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['accuracy'],\n",
        "            label='Train',\n",
        "            color='black')\n",
        "\n",
        "    plt.plot(history.history['val_accuracy'],\n",
        "            label='Val',\n",
        "            color='red')\n",
        "\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "    if flag_savefig:\n",
        "    \tplt.savefig(\"acc_graph_\" + history.model.name + \".png\")\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbALamaCiaA1"
      },
      "source": [
        "# Print execution time\n",
        "def print_execution_time(time_start):\n",
        "    time_elapsed = time.perf_counter() - time_start\n",
        "    min, sec = divmod(time_elapsed, 60)\n",
        "    hour, min = divmod(min, 60)\n",
        "\n",
        "    print(f\"Execution time: {hour:.0f} hour {min:.0f} min {sec:.0f} sec\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo2_WEpT_qQ_"
      },
      "source": [
        "# 3.Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PK1efK55oYo"
      },
      "source": [
        "# For CSF3 (UoM) setting\n",
        "import platform\n",
        "system_name = platform.system()\n",
        "print('system_name: ' + system_name)\n",
        "if system_name == \"Linux\":\n",
        "    flag_summary    = False\n",
        "    flag_cm_norm    = False\n",
        "    flag_plot_model = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYkrNawEhkWr"
      },
      "source": [
        "## Read files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czBYeoX9hpQh"
      },
      "source": [
        "# dataset files (train & test)\n",
        "files_train = glob.glob('../data/train/*.dat')\n",
        "files_test = glob.glob('../data/test/*.dat')\n",
        "\n",
        "# Read datafiles (if not yet)\n",
        "if not 'R_dataset_train' in locals():\n",
        "    R_dataset_train, nan_train = read_files(files_train)\n",
        "    R_dataset_test, nan_test = read_files(files_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpHFjOJmJe8j"
      },
      "source": [
        "# Discard null action records\n",
        "if flag_delete_null:\n",
        "    if flag_label == 'Locomotion':\n",
        "        dataset_train = R_dataset_train[R_dataset_train.iloc[:, 243] != 0]\n",
        "        dataset_test = R_dataset_test[R_dataset_test.iloc[:, 243] != 0]\n",
        "    elif flag_label == 'ML_Both_Arms':\n",
        "        dataset_train = R_dataset_train[R_dataset_train.iloc[:, 249] != 0]\n",
        "        dataset_test = R_dataset_test[R_dataset_test.iloc[:, 249] != 0]\n",
        "else:\n",
        "    dataset_train = R_dataset_train\n",
        "    dataset_test = R_dataset_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNztxnO9NAEw"
      },
      "source": [
        "# Balancing data 1 (After reading files)\n",
        "if flag_balance_under1:\n",
        "    if flag_label == 'Locomotion':\n",
        "        idx_label = 243\n",
        "    elif flag_label == 'ML_Both_Arms':\n",
        "        idx_label = 249\n",
        "\n",
        "    min_train = dataset_train.iloc[:, idx_label].value_counts().min()\n",
        "    dataset_train_np = dataset_train.to_numpy()\n",
        "\n",
        "    for i in dataset_train.iloc[:, idx_label].unique():\n",
        "        dataset_train_np = np.delete(dataset_train_np, np.where(dataset_train_np[:, idx_label] == i)[0][min_train:], axis=0)\n",
        "\n",
        "    dataset_train = pd.DataFrame(dataset_train_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtjVP0_aiGaE"
      },
      "source": [
        "## Divide X / Y\n",
        "(features and labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCpGpPVZ7hFE"
      },
      "source": [
        "## Features (X)\n",
        "# Strip unnecessay columns\n",
        "# (following opportunity challenge specification)\n",
        "X_train = pd.concat([\n",
        "    dataset_train.iloc[:, 1:46], # (included:excluded)\n",
        "    dataset_train.iloc[:, 50:59],\n",
        "    dataset_train.iloc[:, 63:72],\n",
        "    dataset_train.iloc[:, 76:85],\n",
        "    dataset_train.iloc[:, 89:98],\n",
        "    dataset_train.iloc[:, 102:134]],\n",
        "    axis=1)\n",
        "\n",
        "X_test = pd.concat([\n",
        "    dataset_test.iloc[:, 1:46],\n",
        "    dataset_test.iloc[:, 50:59],\n",
        "    dataset_test.iloc[:, 63:72],\n",
        "    dataset_test.iloc[:, 76:85],\n",
        "    dataset_test.iloc[:, 89:98],\n",
        "    dataset_test.iloc[:, 102:134]],\n",
        "    axis=1)\n",
        "\n",
        "## Labels (Y)\n",
        "# from last 7 columns\n",
        "if flag_label == 'Locomotion':\n",
        "    y_train = dataset_train.iloc[:,243]\n",
        "    y_test = dataset_test.iloc[:,243]\n",
        "elif flag_label == 'ML_Both_Arms':\n",
        "    y_train = dataset_train.iloc[:,249]\n",
        "    y_test = dataset_test.iloc[:,249]\n",
        "\n",
        "y_train = y_train.rename('Label')\n",
        "y_test = y_test.rename('Label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByRz5SAuLtYZ"
      },
      "source": [
        "num_features = len(X_train.columns)\n",
        "\n",
        "# Input shape of NNs\n",
        "cnn_1d_input_shape = (window_size, num_features)\n",
        "lstm_input_shape = (window_size, num_features)\n",
        "cnn1d_lstm_input_shape = (None, cnn_lstm_length, num_features)\n",
        "convlstm_input_shape = (convlstm_steps, 1, convlstm_length, num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWzfyAgh1pYe"
      },
      "source": [
        "## (Distributions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHKaP4TEzu4p"
      },
      "source": [
        "if flag_data_dist:\n",
        "    plot_variable_distributions(X_train.to_numpy(), start=0, end=29)\n",
        "    plot_variable_distributions(X_train.to_numpy(), start=30, end=59)\n",
        "    plot_variable_distributions(X_train.to_numpy(), start=60, end=89)\n",
        "    plot_variable_distributions(X_train.to_numpy(), start=90, end=112)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iobla8gqlIMr"
      },
      "source": [
        "## Encode labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHjz98a9lLh5"
      },
      "source": [
        "## Encode label (one-hot)\n",
        "# Adjust label values for to_categorical()\n",
        "y_train = adjust_idx_labels(y_train)\n",
        "y_test = adjust_idx_labels(y_test)\n",
        "\n",
        "# Convert class vector (int) to one-hot vector\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZLhGzvM--Jz"
      },
      "source": [
        "## Split train / val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWAxF-y5H5tF"
      },
      "source": [
        "# Split into train and val (No shuffle)\n",
        "X_train, X_val, y_train, y_val = \\\n",
        "    train_test_split(X_train, y_train,\n",
        "                     train_size=ratio_train,\n",
        "                     random_state=seed,\n",
        "                     shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsMQisfLhHU3"
      },
      "source": [
        "# Balancing data 2 (After splitting train, val, and test)\n",
        "if flag_balance_under2:\n",
        "    min_train = y_train.value_counts().min()\n",
        "\n",
        "    X_train = X_train.to_numpy()\n",
        "    y_train = y_train.to_numpy()\n",
        "\n",
        "    y_train_n = y_train.argmax(axis=-1)\n",
        "\n",
        "    for i in range(len(np.unique(y_train_n))):\n",
        "        X_train = np.delete(X_train, np.where(y_train_n == i)[0][min_train:], axis=0)\n",
        "        y_train_n = np.delete(y_train_n, np.where(y_train_n == i)[0][min_train:], axis=0)\n",
        "\n",
        "    y_train = to_categorical(y_train_n)\n",
        "\n",
        "    X_train = pd.DataFrame(X_train)\n",
        "    y_train = pd.DataFrame(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWVCcwgqz5xe"
      },
      "source": [
        "# The number of classes\n",
        "num_classes = len(y_train.columns)\n",
        "\n",
        "# label list (for classification_report, confusion_matrix)\n",
        "if flag_label == 'Locomotion':\n",
        "    labels_cr = labels_Loco\n",
        "    labels_cm = labels_Loco\n",
        "elif flag_label == 'ML_Both_Arms':\n",
        "    labels_cr = labels_ML\n",
        "    labels_cm = labels_ML\n",
        "\n",
        "if flag_delete_null:\n",
        "     labels_cr = np.delete(labels_cr, 0)\n",
        "#     labels_cm = np.delete(labels_cm, 0)\n",
        "\n",
        "# confusion_matrix\n",
        "labels = np.arange(0, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJCpre3SCZJz"
      },
      "source": [
        "## Scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dBhtR30CzES"
      },
      "source": [
        "if flag_scaling == \"Norm\":\n",
        "    scaler = MinMaxScaler()\n",
        "elif flag_scaling == \"Std\":\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data (to avoid data leakage)\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Scale (to numpy)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to numpy\n",
        "y_train = y_train.to_numpy()\n",
        "y_val = y_val.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg7Tg528bf_q"
      },
      "source": [
        "## (Distributions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxzsrf_Kiorm"
      },
      "source": [
        "if flag_data_dist:\n",
        "    plot_variable_distributions(X_train, start=0, end=29)\n",
        "    plot_variable_distributions(X_train, start=30, end=59)\n",
        "    plot_variable_distributions(X_train, start=60, end=89)\n",
        "    plot_variable_distributions(X_train, start=90, end=112)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiOPY-WnW8Ra"
      },
      "source": [
        "## Sliding window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJNJ-7CHW_xF"
      },
      "source": [
        "X_train_sw, y_train_sw = opp_sliding_window(X_train, y_train)\n",
        "X_val_sw, y_val_sw  = opp_sliding_window(X_val, y_val)\n",
        "X_test_sw, y_test_sw  = opp_sliding_window(X_test, y_test)\n",
        "\n",
        "if flag_sw_label == \"last\":\n",
        "    # last class of each sliding window\n",
        "    y_train_sw_label = np.asarray([[i[-1]] for i in y_train_sw]).reshape(-1, y_train_sw.shape[-1])\n",
        "    y_val_sw_label = np.asarray([[i[-1]] for i in y_val_sw]).reshape(-1, y_val_sw.shape[-1])\n",
        "    y_test_sw_label = np.asarray([[i[-1]] for i in y_test_sw]).reshape(-1, y_test_sw.shape[-1])\n",
        "\n",
        "elif flag_sw_label == \"mode\":\n",
        "    # mode in each sliding window\n",
        "    y_train_sw_mode = np.asarray([collections.Counter(i.argmax(axis=-1)).most_common()[0][0] for i in y_train_sw])\n",
        "    y_train_sw_label = to_categorical(y_train_sw_mode)\n",
        "    y_val_sw_mode = np.asarray([collections.Counter(i.argmax(axis=-1)).most_common()[0][0] for i in y_val_sw])\n",
        "    y_val_sw_label = to_categorical(y_val_sw_mode)\n",
        "    y_test_sw_mode = np.asarray([collections.Counter(i.argmax(axis=-1)).most_common()[0][0] for i in y_test_sw])\n",
        "    y_test_sw_label = to_categorical(y_test_sw_mode)\n",
        "\n",
        "# For evaluation\n",
        "y_test_classes_sw_label = y_test_sw_label.argmax(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLegdea9vGe"
      },
      "source": [
        "# Blancing data 3 (After sliding window)\n",
        "if flag_balance_under3:\n",
        "    y_train_sw_n = y_train_sw_label.argmax(axis=-1)\n",
        "    min_train = pd.DataFrame(y_train_sw_label).value_counts().min()\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        X_train_sw = np.delete(X_train_sw, np.where(y_train_sw_n == i)[0][min_train:], axis=0)\n",
        "        y_train_sw_n = np.delete(y_train_sw_n, np.where(y_train_sw_n == i)[0][min_train:], axis=0)\n",
        "\n",
        "    y_train_sw_label = to_categorical(y_train_sw_n)\n",
        "\n",
        "elif flag_balance_over3:\n",
        "    y_train_sw_n = y_train_sw_label.argmax(axis=-1)\n",
        "    max_train = pd.DataFrame(y_train_sw_n)[0].value_counts().max()\n",
        "    num_labels = np.unique(y_train_sw_n).size\n",
        "\n",
        "    X_train_sw_balanced = np.empty((num_labels * max_train, X_train_sw.shape[1], X_train_sw.shape[2]))\n",
        "    y_train_sw_balanced = np.empty((num_labels * max_train, y_train_sw.shape[1], y_train_sw.shape[2]))\n",
        "    y_train_sw_label_balanced = np.empty((num_labels * max_train, y_train_sw_label.shape[1]))\n",
        "\n",
        "    X_train_sw_balanced[:X_train_sw.shape[0]] = X_train_sw\n",
        "    y_train_sw_balanced[:y_train_sw.shape[0]] = y_train_sw\n",
        "    y_train_sw_label_balanced[:y_train_sw_label.shape[0]] = y_train_sw_label\n",
        "\n",
        "    l = X_train_sw.shape[0]\n",
        "    for c in np.unique(y_train_sw_n):\n",
        "        num = np.count_nonzero(y_train_sw_n == c)\n",
        "        if max_train > num:\n",
        "            num_diff = max_train - num\n",
        "            idx_c = np.where(y_train_sw_n == c)[0]\n",
        "            idx_add = np.random.choice(idx_c, num_diff, replace=True)        \n",
        "            for i in idx_add:\n",
        "                X_train_sw_balanced[l] = X_train_sw[i]\n",
        "                y_train_sw_balanced[l] = y_train_sw[i]\n",
        "                y_train_sw_label_balanced[l] = y_train_sw_label[i]\n",
        "                l += 1\n",
        "\n",
        "    X_train_sw = X_train_sw_balanced\n",
        "    y_train_sw = y_train_sw_balanced\n",
        "    y_train_sw_label = y_train_sw_label_balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SkfXm4eYEXG"
      },
      "source": [
        "## [Summary]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3wLGu7vQiEd"
      },
      "source": [
        "if flag_summary:\n",
        "    # The number of samples (train, val, test)\n",
        "    num_samples = len(X_train) + len(X_val) + len(X_test)\n",
        "    num_train = X_train.shape[0]\n",
        "    num_val = X_val.shape[0]\n",
        "    num_test = X_test.shape[0]\n",
        "\n",
        "    num_classes_train = y_train.shape[-1]\n",
        "    num_classes_val = y_val.shape[-1]\n",
        "    num_classes_test = y_test.shape[-1]\n",
        "\n",
        "    y_counts = pd.concat([np.flip(pd.DataFrame(y_train).value_counts(sort=False)),\n",
        "                        np.flip(pd.DataFrame(y_val).value_counts(sort=False)),\n",
        "                        np.flip(pd.DataFrame(y_test).value_counts(sort=False))],\n",
        "                        axis=1)\n",
        "\n",
        "    y_counts = y_counts.style.hide_index().highlight_null('red').set_precision(0)\n",
        "\n",
        "    y_counts_sw = pd.concat([np.flip(pd.DataFrame(y_train_sw_label).value_counts(sort=False)),\n",
        "                        np.flip(pd.DataFrame(y_val_sw_label).value_counts(sort=False)),\n",
        "                        np.flip(pd.DataFrame(y_test_sw_label).value_counts(sort=False))],\n",
        "                    axis=1)\n",
        "\n",
        "    y_counts_sw = y_counts_sw.style.hide_index().highlight_null('red').set_precision(0)\n",
        "\n",
        "    print('[# of samples]')\n",
        "    print(f'Total: {num_samples:>7,}')\n",
        "    print(f'Train: {num_train:>7,}')\n",
        "    print(f'Val:   {num_val:>7,}')\n",
        "    print(f'Test:  {num_test:>7,}')\n",
        "    print()\n",
        "\n",
        "    print('[After sliding window]')\n",
        "    print(f'Train: {X_train_sw.shape[0]:>7,}')\n",
        "    print(f'Val:   {X_val_sw.shape[0]:>7,}')\n",
        "    print(f'Test:  {X_test_sw.shape[0]:>7,}')\n",
        "    print()\n",
        "\n",
        "    print('[# of features]')\n",
        "    print(f'{num_features}')\n",
        "    print()\n",
        "\n",
        "    print('[# of classes]')\n",
        "    print(f'Total: {num_classes}')\n",
        "    print(f'Train: {num_classes_train}')\n",
        "    print(f'Val:   {num_classes_val}')\n",
        "    print(f'Test:  {num_classes_test}')\n",
        "    print()\n",
        "\n",
        "    print('[Original data]')\n",
        "    print('Train   Val   Test')\n",
        "    display(y_counts)\n",
        "    print()\n",
        "\n",
        "    print('[After sliding window]')\n",
        "    print('Train   Val   Test')\n",
        "    display(y_counts_sw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywOIh_-nCzjS"
      },
      "source": [
        "# 4.Train & Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIpQ3qHFUCK0"
      },
      "source": [
        "## callbacks\n",
        "cb = []\n",
        "\n",
        "# EarlyStopping\n",
        "if flag_EarlyStopping:\n",
        "    es = EarlyStopping(monitor=flag_es_monitor, patience=es_patience)\n",
        "    cb.append(es)\n",
        "\n",
        "# TensorBoard\n",
        "if flag_TensorBoard:\n",
        "    log_dir = dir_log + '/tb/' + datetime.utcnow().strftime('%Y%m%d-%H%M%S')\n",
        "    tb = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    cb.append(tb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWGtwwUjIogZ"
      },
      "source": [
        "# Train, val, test sets\n",
        "X_train = X_train_sw\n",
        "X_val = X_val_sw\n",
        "X_test = X_test_sw\n",
        "\n",
        "y_train = y_train_sw_label\n",
        "y_val = y_val_sw_label\n",
        "y_test = y_test_sw_label\n",
        "\n",
        "y_test_classes = y_test_classes_sw_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b51cxnB1P49V"
      },
      "source": [
        "## CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF7MUe6YJDB4"
      },
      "source": [
        "if flag_CNN_1d:\n",
        "    if flag_model_load:\n",
        "        model_cnn_1d = load_model(dir_model + \"/\" + modelname_cnn_1d + \".h5\")\n",
        "    else:\n",
        "        model_cnn_1d = build_model_cnn_1d()\n",
        "    model_cnn_1d.summary()\n",
        "\n",
        "    if flag_plot_model:\n",
        "        plot_model(model_cnn_1d, show_shapes=True, to_file='model_' + model_cnn_1d.name + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wncHGASFJJpP"
      },
      "source": [
        "if flag_CNN_1d:\n",
        "    if not flag_model_load:\n",
        "        time_start = time.perf_counter()\n",
        "\n",
        "        history_cnn_1d = model_cnn_1d.fit(\n",
        "            x               = X_train,\n",
        "            y               = y_train,\n",
        "            batch_size      = batch_size,\n",
        "            epochs          = epochs,\n",
        "            verbose         = 1,\n",
        "            callbacks       = [cb],\n",
        "            validation_data = (X_val, y_val)\n",
        "        )\n",
        "\n",
        "        print_execution_time(time_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI8HzEqJJj8g"
      },
      "source": [
        "if flag_CNN_1d:\n",
        "    if not flag_model_load:\n",
        "        plot_acc_graph(history_cnn_1d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU8QO_166nU9"
      },
      "source": [
        "if flag_CNN_1d:\n",
        "    if flag_model_save:\n",
        "        time_current = datetime.utcnow().strftime('%Y%m%d-%H%M%S')\n",
        "        model_cnn_1d.save(dir_model + '/' + model_cnn_1d.name + '_' + time_current + \".h5\", overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNzrQwlFJmJ3"
      },
      "source": [
        "if flag_CNN_1d:\n",
        "    y_pred_classes = model_cnn_1d.predict(X_test).argmax(axis=-1)\n",
        "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    print(f'F1 score(weighted) {f1:.3f}\\n')\n",
        "    print(classification_report(y_test_classes, y_pred_classes, target_names=labels_cr))\n",
        "\n",
        "    if flag_cm_norm:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels, normalize='true')\n",
        "        cm = cm * 100\n",
        "    else:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels)\n",
        "\n",
        "    df_cm = pd.DataFrame(cm, index=labels_cm, columns=labels_cm)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(df_cm, square=True, annot=True, cbar=False, fmt='.0f', cmap='Greens')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    \n",
        "    if flag_savefig:\n",
        "    \tplt.savefig(\"confusion_matrix_\" + model_cnn_1d.name + \".png\")\n",
        "\n",
        "    plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lar3HvcgOY74"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3rv4WYFKdUp"
      },
      "source": [
        "if flag_LSTM_Mto1:\n",
        "    if flag_model_load:\n",
        "        model_lstm_Mto1 = load_model(dir_model + \"/\" + modelname_lstm_Mto1 + \".h5\")\n",
        "    else:\n",
        "        model_lstm_Mto1 = build_model_lstm_Mto1()\n",
        "    model_lstm_Mto1.summary()\n",
        "\n",
        "    if flag_plot_model:\n",
        "        plot_model(model_lstm_Mto1, show_shapes=True, to_file='model_' + model_lstm_Mto1.name + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D7O2TKxKj1X"
      },
      "source": [
        "if flag_LSTM_Mto1:\n",
        "    if not flag_model_load:\n",
        "        time_start = time.perf_counter()\n",
        "\n",
        "        history_lstm_Mto1 = model_lstm_Mto1.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            batch_size      = batch_size,\n",
        "            epochs          = epochs,\n",
        "            verbose         = 1,\n",
        "            callbacks       = [cb],\n",
        "            validation_data = (X_val, y_val)\n",
        "        )\n",
        "\n",
        "        print_execution_time(time_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtkBy54A88Pa"
      },
      "source": [
        "if flag_LSTM_Mto1:\n",
        "    if not flag_model_load:\n",
        "        plot_acc_graph(history_lstm_Mto1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80ibjsrBKtKI"
      },
      "source": [
        "if flag_LSTM_Mto1:\n",
        "    if flag_model_save:\n",
        "        time_current = datetime.utcnow().strftime('%Y%m%d-%H%M%S')\n",
        "        model_lstm_Mto1.save(dir_model + '/' + model_lstm_Mto1.name + '_' + time_current + \".h5\", overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1ET3iF3KwiQ"
      },
      "source": [
        "if flag_LSTM_Mto1:\n",
        "    y_pred_classes = model_lstm_Mto1.predict(X_test).argmax(axis=-1)\n",
        "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    print(f'F1 score(weighted) {f1:.3f}\\n')\n",
        "    print(classification_report(y_test_classes, y_pred_classes, target_names=labels_cr))\n",
        "\n",
        "    if flag_cm_norm:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels, normalize='true')\n",
        "        cm = cm * 100\n",
        "    else:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels)\n",
        "\n",
        "    df_cm = pd.DataFrame(cm, index=labels_cm, columns=labels_cm)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(df_cm, square=True, annot=True, cbar=False, fmt='.0f', cmap='Greens')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    \n",
        "    if flag_savefig:\n",
        "    \tplt.savefig(\"confusion_matrix_\" + model_lstm_Mto1.name + \".png\")\n",
        "\n",
        "    plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0nQ_CJvVUoU"
      },
      "source": [
        "## CNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sm8vFLjEX-g"
      },
      "source": [
        "# Reshape data into 4D for CNN1D-LSTM\n",
        "X_train_cnn1d_lstm = X_train.reshape((X_train.shape[0], cnn_lstm_steps, cnn_lstm_length, X_train.shape[-1]))\n",
        "X_val_cnn1d_lstm = X_val.reshape((X_val.shape[0], cnn_lstm_steps, cnn_lstm_length, X_val.shape[-1]))\n",
        "X_test_cnn1d_lstm = X_test.reshape((X_test.shape[0], cnn_lstm_steps, cnn_lstm_length, X_test.shape[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krVJWV54VWW-"
      },
      "source": [
        "if flag_CNN1D_LSTM:\n",
        "    if flag_model_load:\n",
        "        model_cnn1d_lstm = load_model(dir_model + \"/\" + modelname_cnn1d_lstm + \".h5\")\n",
        "    else:\n",
        "        model_cnn1d_lstm = build_model_cnn1d_lstm()\n",
        "    model_cnn1d_lstm.summary()\n",
        "\n",
        "    if flag_plot_model:\n",
        "        plot_model(model_cnn1d_lstm, show_shapes=True, to_file='model_' + model_cnn1d_lstm.name + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN_0aOG8YoYj"
      },
      "source": [
        "if flag_CNN1D_LSTM:\n",
        "    if not flag_model_load:\n",
        "        time_start = time.perf_counter()\n",
        "\n",
        "        history_cnn1d_lstm = model_cnn1d_lstm.fit(\n",
        "            X_train_cnn1d_lstm,\n",
        "            y_train,\n",
        "            batch_size      = batch_size,\n",
        "            epochs          = epochs,\n",
        "            verbose         = 1,\n",
        "            callbacks       = [cb],\n",
        "            validation_data = (X_val_cnn1d_lstm, y_val)\n",
        "        )\n",
        "\n",
        "        print_execution_time(time_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruqui2vmRUct"
      },
      "source": [
        "if flag_CNN1D_LSTM:\n",
        "    if not flag_model_load:\n",
        "        plot_acc_graph(history_cnn1d_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkbgQ7zo-LVq"
      },
      "source": [
        "if flag_CNN1D_LSTM:\n",
        "    if flag_model_save:\n",
        "        time_current = datetime.utcnow().strftime('%Y%m%d-%H%M%S')\n",
        "        model_cnn1d_lstm.save(dir_model + '/' + model_cnn1d_lstm.name + '_' + time_current + \".h5\", overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRagkHvMRagu"
      },
      "source": [
        "if flag_CNN1D_LSTM:\n",
        "    y_pred_classes = model_cnn1d_lstm.predict(X_test_cnn1d_lstm).argmax(axis=-1)\n",
        "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    print(f'F1 score(weighted) {f1:.3f}\\n')\n",
        "    print(classification_report(y_test_classes, y_pred_classes, target_names=labels_cr))\n",
        "\n",
        "    if flag_cm_norm:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels, normalize='true')\n",
        "        cm = cm * 100\n",
        "    else:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels)\n",
        "\n",
        "    df_cm = pd.DataFrame(cm, index=labels_cm, columns=labels_cm)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(df_cm, square=True, annot=True, cbar=False, fmt='.0f', cmap='Greens')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    \n",
        "    if flag_savefig:\n",
        "    \tplt.savefig(\"confusion_matrix_\" + model_cnn1d_lstm.name + \".png\")\n",
        "\n",
        "    plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CcWqW3NUBR0"
      },
      "source": [
        "## ConvLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny9_LxRbUDCc"
      },
      "source": [
        "# Reshape data into 5D for ConvLSTM\n",
        "X_train_convlstm = X_train.reshape((X_train.shape[0], convlstm_steps, 1, convlstm_length, X_train.shape[-1]))\n",
        "X_val_convlstm = X_val.reshape((X_val.shape[0], convlstm_steps, 1, convlstm_length, X_val.shape[-1]))\n",
        "X_test_convlstm = X_test.reshape((X_test.shape[0], convlstm_steps, 1, convlstm_length, X_test.shape[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_guH2InUrBa"
      },
      "source": [
        "if flag_ConvLSTM:\n",
        "    if flag_model_load:\n",
        "        model_convlstm = load_model(dir_model + \"/\" + modelname_convlstm + \".h5\")\n",
        "    else:\n",
        "        model_convlstm = build_model_convlstm()\n",
        "    model_convlstm.summary()\n",
        "\n",
        "    if flag_plot_model:\n",
        "        plot_model(model_convlstm, show_shapes=True, to_file='model_' + model_convlstm.name + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ibm6HvQWrlB"
      },
      "source": [
        "if flag_ConvLSTM:\n",
        "    if not flag_model_load:\n",
        "        time_start = time.perf_counter()\n",
        "\n",
        "        history_convlstm = model_convlstm.fit(\n",
        "            X_train_convlstm,\n",
        "            y_train,\n",
        "            batch_size      = batch_size,\n",
        "            epochs          = epochs,\n",
        "            verbose         = 1,\n",
        "            callbacks       = [cb],\n",
        "            validation_data = (X_val_convlstm, y_val)\n",
        "        )\n",
        "\n",
        "        print_execution_time(time_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxfj1nTLXqzC"
      },
      "source": [
        "if flag_ConvLSTM:\n",
        "    if not flag_model_load:\n",
        "        plot_acc_graph(history_convlstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7d0xMcB-b9h"
      },
      "source": [
        "if flag_ConvLSTM:\n",
        "    if flag_model_save:\n",
        "        time_current = datetime.utcnow().strftime('%Y%m%d-%H%M%S')\n",
        "        model_convlstm.save(dir_model + '/' + model_convlstm.name + '_' + time_current + \".h5\", overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuUpw2EUXzc7"
      },
      "source": [
        "if flag_ConvLSTM:\n",
        "    y_pred_classes = model_convlstm.predict(X_test_convlstm).argmax(axis=-1)\n",
        "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    print(f'F1 score(weighted) {f1:.3f}\\n')\n",
        "    print(classification_report(y_test_classes, y_pred_classes, target_names=labels_cr))\n",
        "\n",
        "    if flag_cm_norm:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels, normalize='true')\n",
        "        cm = cm * 100\n",
        "    else:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels)\n",
        "\n",
        "    df_cm = pd.DataFrame(cm, index=labels_cm, columns=labels_cm)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(df_cm, square=True, annot=True, cbar=False, fmt='.0f', cmap='Greens')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    \n",
        "    if flag_savefig:\n",
        "    \tplt.savefig(\"confusion_matrix_\" + model_convlstm.name + \".png\")\n",
        "\n",
        "    plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz1ssCrTRmAc"
      },
      "source": [
        "## Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB4jdpkyqnfw"
      },
      "source": [
        "if flag_Ensemble:\n",
        "    # build sub-models\n",
        "    sub_models = list()\n",
        "    X_train_ensemble = list()\n",
        "    X_val_ensemble = list()\n",
        "    X_test_ensemble = list()\n",
        "\n",
        "    if flag_CNN_1d:\n",
        "        sub_models.append(model_cnn_1d)\n",
        "        X_train_ensemble.append(X_train)\n",
        "        X_val_ensemble.append(X_val)\n",
        "        X_test_ensemble.append(X_test)\n",
        "    if flag_LSTM_Mto1:\n",
        "        sub_models.append(model_lstm_Mto1)\n",
        "        X_train_ensemble.append(X_train)\n",
        "        X_val_ensemble.append(X_val)\n",
        "        X_test_ensemble.append(X_test)\n",
        "    if flag_CNN1D_LSTM:\n",
        "        sub_models.append(model_cnn1d_lstm)\n",
        "        X_train_ensemble.append(X_train_cnn1d_lstm)\n",
        "        X_val_ensemble.append(X_val_cnn1d_lstm)\n",
        "        X_test_ensemble.append(X_test_cnn1d_lstm)\n",
        "    if flag_ConvLSTM:\n",
        "        sub_models.append(model_convlstm)\n",
        "        X_train_ensemble.append(X_train_convlstm)\n",
        "        X_val_ensemble.append(X_val_convlstm)\n",
        "        X_test_ensemble.append(X_test_convlstm)\n",
        "\n",
        "    # freeze parameters for sub-models\n",
        "    for i in range(len(sub_models)):\n",
        "        for layer in sub_models[i].layers:\n",
        "            layer.trainable = False\n",
        "            layer._name = 'ensemble_' + sub_models[i].name + '_' + layer.name\n",
        "\n",
        "    inputs = [model.input for model in sub_models]\n",
        "    outputs = [model.output for model in sub_models]\n",
        "\n",
        "    model_ensemble = build_model_ensemble(inputs, outputs)\n",
        "    model_ensemble.summary()\n",
        "\n",
        "    if flag_plot_model:\n",
        "        plot_model(model_ensemble, show_shapes=True, to_file='model_' + model_ensemble.name + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPtmF3VK43hv"
      },
      "source": [
        "if flag_Ensemble:\n",
        "    history_ensemble    = model_ensemble.fit(\n",
        "        x               = X_train_ensemble,\n",
        "        y               = y_train,\n",
        "        batch_size      = batch_size,\n",
        "        epochs          = epochs,\n",
        "        verbose         = 1,\n",
        "        callbacks       = [cb],\n",
        "        validation_data = (X_val_ensemble, y_val)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOVhjDErI8Zp"
      },
      "source": [
        "if flag_Ensemble:\n",
        "    plot_acc_graph(history_ensemble)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh4FArLutRVC"
      },
      "source": [
        "if flag_Ensemble:\n",
        "    y_pred_classes = model_ensemble.predict(X_test_ensemble).argmax(axis=-1)\n",
        "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    print(f'F1 score(weighted) {f1:.3f}\\n')\n",
        "    print(classification_report(y_test_classes, y_pred_classes, target_names=labels_cr))\n",
        "\n",
        "    if flag_cm_norm:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels, normalize='true')\n",
        "        cm = cm * 100\n",
        "    else:\n",
        "        cm = confusion_matrix(y_test_classes, y_pred_classes, labels=labels)\n",
        "\n",
        "    df_cm = pd.DataFrame(cm, index=labels_cm, columns=labels_cm)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    sns.heatmap(df_cm, square=True, annot=True, cbar=False, fmt='.0f', cmap='Greens')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    if flag_savefig:\n",
        "        plt.savefig(\"confusion_matrix_\" + model_ensemble.name + \".png\")\n",
        "\n",
        "    plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnZhVUx0nM0g"
      },
      "source": [
        "# 5.Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXh1grdwmJ--"
      },
      "source": [
        "if flag_experiment:\n",
        "    time_start = time.perf_counter()\n",
        "\n",
        "    print(f'repeats: {repeats}, epochs: {epochs}, batch_size: {batch_size}')\n",
        "    if flag_EarlyStopping:\n",
        "        print(f' EarlyStopping is set (patience: {es_patience})\\n')\n",
        "    else:\n",
        "        print(f' no EarlyStopping\\n')\n",
        "\n",
        "    ## CNN 1D\n",
        "    run_experiment(modelname_cnn_1d,\n",
        "                   X_train, X_val, X_test,\n",
        "                   y_train, y_val, y_test,\n",
        "                   repeats=repeats)\n",
        "\n",
        "    ## LSTM Mto1\n",
        "    run_experiment(modelname_lstm_Mto1,\n",
        "                   X_train, X_val, X_test,\n",
        "                   y_train, y_val, y_test,\n",
        "                   repeats=repeats)\n",
        "\n",
        "    ## CNN1D-LSTM\n",
        "    run_experiment(modelname_cnn1d_lstm,\n",
        "                   X_train_cnn1d_lstm, X_val_cnn1d_lstm, X_test_cnn1d_lstm,\n",
        "                   y_train, y_val, y_test,\n",
        "                   repeats=repeats)\n",
        "\n",
        "    ## ConvLSTM\n",
        "    run_experiment(modelname_convlstm,\n",
        "                   X_train_convlstm, X_val_convlstm, X_test_convlstm,\n",
        "                   y_train, y_val, y_test,\n",
        "                   repeats=repeats)\n",
        "    \n",
        "    print_execution_time(time_start)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}